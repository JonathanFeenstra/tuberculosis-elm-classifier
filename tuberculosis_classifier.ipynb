{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a classifier for detecting tuberculosis in chest X-rays\n",
    "By Jonathan Feenstra (s1024219) and Justin Huberts (s1030368)\n",
    "\n",
    "In this project we attempt to construct a binary Extreme Learning Machine (ELM) classifier to detect tuberculosis in lung X-rays. The U.S. National Library of Medicine has made two datasets of postero-anterior chest radiographs available which we will use to train and test our model: the Montgomery County Set and the Shenzen Set (Jaeger et al., 2014).\n",
    "\n",
    "The datasets are available for download at [https://ceb.nlm.nih.gov/repositories/tuberculosis-chest-x-ray-image-data-sets/](https://ceb.nlm.nih.gov/repositories/tuberculosis-chest-x-ray-image-data-sets/)\n",
    "\n",
    "## Sources\n",
    "- Jaeger, S., Candemir, S., Antani, S., Wáng, Y., Lu, P., & Thoma, G. (2014). Two public chest X-ray datasets for computer-aided screening of pulmonary diseases. _Quantitative Imaging In Medicine And Surgery, 4_(6), 475-477.\n",
    "\n",
    "## Some example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-000007fb7833>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mshen_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshen_tb_neg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mshen_tb_pos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmont_tb_neg_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmont_tb_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mmont_tb_pos_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmont_tb_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mshen_tb_neg_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshen_tb_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Emperor Justinian\\Anaconda\\lib\\random.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, population, k)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m21\u001b[0m        \u001b[1;31m# size of a small set minus size of an empty list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import ndimage\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "mont_tb_neg = glob.glob('Data/MontgomerySet/CXR_png/*_0.png')\n",
    "mont_tb_pos = glob.glob('Data/MontgomerySet/CXR_png/*_1.png')\n",
    "shen_tb_neg = glob.glob('Data/ChinaSet_AllFiles/CXR_png/*_0.png')\n",
    "shen_tb_pos = glob.glob('Data/ChinaSet_AllFiles/CXR_png/*_1.png')\n",
    "\n",
    "mont_set = mont_tb_neg + mont_tb_pos\n",
    "shen_set = shen_tb_neg + shen_tb_pos\n",
    "\n",
    "mont_tb_neg_sample = sample(mont_tb_neg, 5)\n",
    "mont_tb_pos_sample = sample(mont_tb_pos, 5)\n",
    "shen_tb_neg_sample = sample(shen_tb_neg, 5)\n",
    "shen_tb_pos_sample = sample(shen_tb_pos, 5)\n",
    "\n",
    "mont_fig = plt.figure(figsize=(15,8))\n",
    "mont_fig.suptitle('Montgomery County Set', size=18)\n",
    "for i, filename in enumerate(mont_tb_neg_sample + mont_tb_pos_sample):\n",
    "    img = mpimg.imread(filename)\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    if i <= 4:\n",
    "        plt.title('Healthy')\n",
    "    else:\n",
    "        plt.title('Tuberculosis')\n",
    "\n",
    "shen_fig = plt.figure(figsize=(15,8))\n",
    "shen_fig.suptitle('Shenzen Set', size=18)\n",
    "for i, filename in enumerate(shen_tb_neg_sample + shen_tb_pos_sample):\n",
    "    img = mpimg.imread(filename)\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(img)\n",
    "    if i <= 4:\n",
    "        plt.title('Healthy')\n",
    "    else:\n",
    "        plt.title('Tuberculosis')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Since all Montgomery County images have a default size of either 4,020×4,892 or 4,892×4,020 pixels, while the actual X-ray size varies, some images contain empty space (black background). For the Shenzen Set this is not the case, but here the image sizes vary more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### Step 1: Trim the Montgomery set images to remove empty space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "\n",
    "# This code was used to trim the images:\n",
    "\n",
    "# def trim(im):\n",
    "#     bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n",
    "#     diff = ImageChops.difference(im, bg)\n",
    "#     diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "#     bbox = diff.getbbox()\n",
    "#     if bbox:\n",
    "#         return im.crop(bbox)\n",
    "\n",
    "# for filename in mont_set:\n",
    "#     trim(Image.open(filename)).save(filename.replace('CXR_png', 'Trimmed'))\n",
    "\n",
    "mont_set = glob.glob('Data/MontgomerySet/Trimmed/*.png')\n",
    "\n",
    "trim_fig = plt.figure(figsize=(15,8))\n",
    "trim_fig.suptitle('Montgomery County Trimmed', size=18)\n",
    "for i, filename in enumerate(mont_tb_neg_sample + mont_tb_pos_sample):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(np.array(Image.open(filename.replace('CXR_png', 'Trimmed'))), cmap='gray')\n",
    "    if i <= 4:\n",
    "        plt.title('Healthy')\n",
    "    else:\n",
    "        plt.title('Tuberculosis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Now that the empty spaces of the images are trimmed away, the Montgomery County image sizes vary as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compress all images to 1024 x 1024 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was used to compress the images:\n",
    "\n",
    "# for i, filename in enumerate(mont_set):\n",
    "#     Image.open(filename).resize((1024,1024),Image.ANTIALIAS).save(filename.replace('Trimmed', 'Compressed'))\n",
    "    \n",
    "# for i, filename in enumerate(shen_set):\n",
    "#     Image.open(filename).resize((1024,1024),Image.ANTIALIAS).save(filename.replace('CXR_png', 'Compressed'))\n",
    "\n",
    "mont_fig = plt.figure(figsize=(15,8))\n",
    "mont_fig.suptitle('Montgomery County Compressed', size=18)\n",
    "for i, filename in enumerate(mont_tb_neg_sample + mont_tb_pos_sample):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(np.array(Image.open(filename.replace('CXR_png', 'Compressed'))), cmap='gray')\n",
    "    if i <= 4:\n",
    "        plt.title('Healthy')\n",
    "    else:\n",
    "        plt.title('Tuberculosis')\n",
    "\n",
    "shen_fig = plt.figure(figsize=(15,8))\n",
    "shen_fig.suptitle('Shenzen Compressed', size=18)\n",
    "for i, filename in enumerate(shen_tb_neg_sample + shen_tb_pos_sample):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(np.array(Image.open(filename.replace('CXR_png', 'Compressed')).convert('L')), cmap='gray')\n",
    "    if i <= 4:\n",
    "        plt.title('Healthy')\n",
    "    else:\n",
    "        plt.title('Tuberculosis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "As expected, all images are now 1024 x 1024 pixels, which has changed the horizontal/vertical scale a bit for all images that were not square-shaped. Hopefully this does not negatively affect the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Invert colors to prepare the images for bone shadow suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was used to invert colors:\n",
    "\n",
    "# import os\n",
    "# import PIL.ImageOps\n",
    "\n",
    "# for filename in glob.glob('Data/MontgomerySet/Compressed/*.png') + glob.glob('Data/ChinaSet_AllFiles/Compressed/*.png'):\n",
    "#     PIL.ImageOps.invert(Image.open(filename).convert('L')).save('Data/Inverted/' + os.path.basename(filename))\n",
    "\n",
    "mont_fig = plt.figure(figsize=(15,8))\n",
    "mont_fig.suptitle('Montgomery County Inverted', size=18)\n",
    "for i, filename in enumerate(mont_tb_neg_sample + mont_tb_pos_sample):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(np.array(Image.open(filename.replace('MontgomerySet/CXR_png', 'Inverted'))), cmap='gray')\n",
    "    if i <= 4:\n",
    "        plt.title('Healthy')\n",
    "    else:\n",
    "        plt.title('Tuberculosis')\n",
    "    \n",
    "shen_fig = plt.figure(figsize=(15,8))\n",
    "shen_fig.suptitle('Shenzen Inverted', size=18)\n",
    "for i, filename in enumerate(shen_tb_neg_sample + shen_tb_pos_sample):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(np.array(Image.open(filename.replace('ChinaSet_AllFiles/CXR_png', 'Inverted')).convert('L')), cmap='gray')\n",
    "    if i <= 4:\n",
    "        plt.title('Healthy')\n",
    "    else:\n",
    "        plt.title('Tuberculosis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "The colors were inverted properly, which has made the lungs brighter than the other parts of the chest. Remarkably, the lungs in the Montgomery County Set appear to look brighter than those in the Shenzen Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply bone shadow suppression\n",
    "For this step we used the pretrained model from  [https://github.com/hmchuong/ML-BoneSuppression](https://github.com/hmchuong/ML-BoneSuppression) to suppress the bone shadows of the inverted images using the following shell script (BASH):\n",
    "```\n",
    "#!/bin/bash\n",
    "for filename in ./Data/Inverted/*\n",
    "do\n",
    "    python test.py --input \\$filename --output \"\\${filename/Inverted/Suppressed}\"\n",
    "done\n",
    "```\n",
    "Note that executing this script requires some of the files from the [GitHub repository](https://github.com/hmchuong/ML-BoneSuppression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mont_fig = plt.figure(figsize=(15,8))\n",
    "mont_fig.suptitle('Montgomery County Suppressed', size=18)\n",
    "for i, filename in enumerate(mont_tb_neg_sample + mont_tb_pos_sample):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(np.array(Image.open(filename.replace('MontgomerySet/CXR_png', 'Suppressed'))), cmap='gray')\n",
    "    if i <= 4:\n",
    "        plt.title('Healthy')\n",
    "    else:\n",
    "        plt.title('Tuberculosis')\n",
    "    \n",
    "shen_fig = plt.figure(figsize=(15,8))\n",
    "shen_fig.suptitle('Shenzen Suppressed', size=18)\n",
    "for i, filename in enumerate(shen_tb_neg_sample + shen_tb_pos_sample):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(np.array(Image.open(filename.replace('ChinaSet_AllFiles/CXR_png', 'Suppressed'))), cmap='gray')\n",
    "    if i <= 4:\n",
    "        plt.title('Healthy')\n",
    "    else:\n",
    "        plt.title('Tuberculosis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "The bone shadows have significantly been suppressed, especially for the Montgomery County Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Reconstruct the images using an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Emperor Justinian\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 529 samples, validate on 67 samples\n",
      "Epoch 1/1\n",
      "529/529 [==============================] - 87s 165ms/step - loss: -77.1976 - val_loss: -507.1137\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have 2 dimensions, but got array with shape (67, 1024, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e2f46493375e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m# note that we take them from the *test* set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[0mencoded_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m \u001b[0mdecoded_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\Emperor Justinian\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Emperor Justinian\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Emperor Justinian\\Anaconda\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_2 to have 2 dimensions, but got array with shape (67, 1024, 32)"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "#scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(1024, 1024))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(1024, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "#mont_set.sort(key=lambda x: x.filename, reverse=True)\n",
    "#random.seed(42)\n",
    "#random.shuffle(mont_set) # shuffles the ordering of the data\n",
    "\n",
    "#\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "mont_set = []\n",
    "\n",
    "#mont_set.sort(key=lambda x: x.filename, reverse=True)\n",
    "random.seed(42)\n",
    "random.shuffle(mont_set) # shuffles the ordering of the data\n",
    "\n",
    "for filename in glob.glob('Data/Suppressed/*.png'):\n",
    "    img = np.frombuffer(Image.open(filename).tobytes(), dtype=np.uint8)\n",
    "    if os.path.basename(filename)[0] == 'C': #M\n",
    "        mont_set.append(img)\n",
    "        \n",
    "\n",
    "#        \n",
    "        \n",
    "for index, img in enumerate(mont_set):\n",
    "    img = np.frombuffer(img.tobytes(), dtype=np.uint8)\n",
    "    img = img.reshape((1024, 1024))  \n",
    "    mont_set[index] = img\n",
    "\n",
    "split_1 = int(0.8 * len(mont_set))\n",
    "split_2 = int(0.9 * len(mont_set))\n",
    "x_train = np.array(mont_set[:split_1])\n",
    "#dev_filenames = mont_set[split_1:split_2]\n",
    "x_test = np.array(mont_set[split_2:])\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "\n",
    "# x_train = x_train[:0,:]\n",
    "# x_test = x_test[:0,:]\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=1, #50\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEYCAYAAACKkJnLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPI7j9I4Lj+M4vePC4zhO77jwOI7TO/8f+7+hLXu5jC4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import reshape\n",
    "\n",
    "imgd = encoded_imgs[-0]\n",
    "print(imgd.shape)\n",
    "\n",
    "I = reshape(imgd,((1024, 32)))\n",
    "plt.imshow(I, extent=(32,1024,32,1024));\n",
    "plt.title(\"img\");\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1024, 1024)\n",
      "(800,)\n",
      "(536, 1024, 1024)\n",
      "(264, 1024, 1024)\n",
      "(536,)\n",
      "(264,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_17: expected ndim=4, found ndim=3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5e27c0420328>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#add model layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;31m#model.add(LeakyReLU(alpha=0.05))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Emperor Justinian\\Anaconda\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    163\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Emperor Justinian\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Emperor Justinian\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_17: expected ndim=4, found ndim=3"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, LeakyReLU, Activation\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# mont_set = []\n",
    "\n",
    "# #mont_set.sort(key=lambda x: x.filename, reverse=True)\n",
    "# random.seed(42)\n",
    "# random.shuffle(mont_set) # shuffles the ordering of the data\n",
    "\n",
    "# for filename in glob.glob('Data/Suppressed/*.png'):\n",
    "#     img = np.frombuffer(Image.open(filename).tobytes(), dtype=np.uint8)\n",
    "#     if os.path.basename(filename)[0] == 'C': #M\n",
    "#         mont_set.append(img)\n",
    "               \n",
    "        \n",
    "# for index, img in enumerate(mont_set):\n",
    "#     img = np.frombuffer(img.tobytes(), dtype=np.uint8)\n",
    "#     img = img.reshape((1024, 1024))  \n",
    "#     mont_set[index] = img\n",
    "\n",
    "# split_1 = int(0.8 * len(mont_set))\n",
    "# split_2 = int(0.9 * len(mont_set))\n",
    "# x_train = np.array(mont_set[:split_1])\n",
    "# x_test = np.array(mont_set[split_2:])\n",
    "\n",
    "mont_images = []\n",
    "shen_images = []\n",
    "mont_labels = []\n",
    "shen_labels = []\n",
    "\n",
    "for filename in glob.glob('Data/Suppressed/*.png'):\n",
    "    img = np.frombuffer(Image.open(filename).tobytes(), dtype=np.uint8)\n",
    "    if os.path.basename(filename)[0] == 'C':\n",
    "        img = img.reshape((1024, 1024)) \n",
    "        shen_images.append(img)\n",
    "        shen_labels.append(int(os.path.splitext(filename)[0][-1]))\n",
    "    elif os.path.basename(filename)[0] == 'M':\n",
    "        img = img.reshape((1024, 1024)) \n",
    "        mont_images.append(img)\n",
    "        mont_labels.append(int(os.path.splitext(filename)[0][-1]))        \n",
    "           \n",
    "# Ik test de ELM hier op alle data om te controleren of het goed werkt\n",
    "X = np.array(mont_images + shen_images)\n",
    "y = np.array(mont_labels + shen_labels, dtype=np.int64)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, input_shape=(1024,1024)))\n",
    "#model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(Conv2D(32, kernel_size=3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)\n",
    "\n",
    "#predict first 4 images in the test set\n",
    "print(model.predict(X_test[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELM classifier\n",
    "Train and test a binary ELM classifier using the extension for scikit-learn. Evaluate it for both of the datasets with an ROC-curve and a confusion matrix.\n",
    "\n",
    "Useful article: [https://ieeexplore.ieee.org/document/7140733](https://ieeexplore.ieee.org/document/7140733)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Emperor Justinian\\Anaconda\\lib\\site-packages\\hpelm\\nnets\\slfn.py:62: RuntimeWarning: overflow encountered in exp\n",
      "  self.func[\"sigm\"] = lambda X, W, B: 1 / (1 + np.exp(np.dot(X, W) + B))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix is not full rank; solving with SVD (slow)\n",
      "This happened because you have duplicated or too many neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Emperor Justinian\\Anaconda\\lib\\site-packages\\hpelm\\nnets\\slfn_python.py:65: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  B = np.linalg.lstsq(HH, HT)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEKCAYAAACGzUnMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsJzgzKywnODMrLD+P5cKzhvIuLBOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hpelm import ELM\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "mont_images = []\n",
    "shen_images = []\n",
    "mont_labels = []\n",
    "shen_labels = []\n",
    "\n",
    "for filename in glob.glob('Data/Suppressed/*.png'):\n",
    "    img = np.frombuffer(Image.open(filename).resize((256,256),Image.ANTIALIAS).tobytes(), dtype=np.uint8)\n",
    "    if os.path.basename(filename)[0] == 'C':\n",
    "        shen_images.append(img)\n",
    "        shen_labels.append(int(os.path.splitext(filename)[0][-1]))\n",
    "    elif os.path.basename(filename)[0] == 'M':\n",
    "        mont_images.append(img)\n",
    "        mont_labels.append(int(os.path.splitext(filename)[0][-1]))\n",
    "\n",
    "# Ik test de ELM hier op alle data om te controleren of het goed werkt\n",
    "X = np.array(mont_images + shen_images)\n",
    "y = np.array(mont_labels + shen_labels, dtype=np.int64)\n",
    "\n",
    "elm = ELM(65536, 1)\n",
    "\n",
    "elm.add_neurons(100, 'sigm')\n",
    "elm.add_neurons(50, 'lin')\n",
    "elm.add_neurons(2, 'sigm')\n",
    "\n",
    "# Train en test data moet nog worden gesplitst\n",
    "elm.train(X, y)\n",
    "y_pred = np.array(elm.predict(X), dtype=np.int64)\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y, y_pred), colorbar=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# TODO: k-fold cross validation (k=10), plot ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
